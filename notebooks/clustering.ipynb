{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Dataset Clustering\n",
    "This notebook aims to cluster scholarly articles from the Arxiv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-ph</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>calculation prompt diphoton production section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>sparsity certifying decomposition algorithm el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>evolution earth moon dark matter field fluid e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>math.CO</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>determinant stirling cycle number count unlabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>dyadic lambda alpha lambda alpha compute lambd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        categories update_date  \\\n",
       "0           hep-ph  2008-11-26   \n",
       "1    math.CO cs.CG  2008-12-13   \n",
       "2   physics.gen-ph  2008-01-13   \n",
       "3          math.CO  2007-05-23   \n",
       "4  math.CA math.FA  2013-10-15   \n",
       "\n",
       "                                                text  \n",
       "0  calculation prompt diphoton production section...  \n",
       "1  sparsity certifying decomposition algorithm el...  \n",
       "2  evolution earth moon dark matter field fluid e...  \n",
       "3  determinant stirling cycle number count unlabe...  \n",
       "4  dyadic lambda alpha lambda alpha compute lambd...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = '../data/data_preprocessed.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df):\n",
    "    # Replace NaN values in the 'text' column with empty strings\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    # Instantiate TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    return X, vectorizer\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words:\n",
      "                        words         freq\n",
      "894           neural network  2956.289274\n",
      "745           magnetic field  2903.412315\n",
      "101               black hole  2484.004071\n",
      "736         machine learning  1889.475077\n",
      "245              dark matter  1706.234735\n",
      "854              monte carlo  1606.282571\n",
      "278            deep learning  1527.605227\n",
      "304    differential equation  1276.674484\n",
      "535                gamma ray  1128.579621\n",
      "1003               power law  1119.474131\n",
      "287           degree freedom  1103.094287\n",
      "814               mean field  1028.609843\n",
      "1132  reinforcement learning  1000.080371\n",
      "112       boundary condition   971.083950\n",
      "1152         result obtained   963.345883\n",
      "1171            scalar field   930.532231\n",
      "1316          star formation   916.473632\n",
      "574       gravitational wave   875.489761\n",
      "1070        quantum mechanic   856.858586\n",
      "696              lie algebra   835.680642\n",
      "Data samples: 228812\n"
     ]
    }
   ],
   "source": [
    "# Sample a fraction of your data (e.g., 10%)\n",
    "sample_fraction = 0.1\n",
    "df_sample = df.sample(frac=sample_fraction)\n",
    "\n",
    "# Vectorize the sampled text data:\n",
    "vectorizer_tfidf = TfidfVectorizer(norm=\"l2\", analyzer=\"word\", ngram_range=(2,3), min_df=5, max_df=0.99, max_features=1500, lowercase=False)\n",
    "tfidf_vectors = vectorizer_tfidf.fit_transform(df_sample['text'])\n",
    "tfidf_terms = vectorizer_tfidf.get_feature_names_out()\n",
    "\n",
    "# Looking for most frequent words\n",
    "sums = tfidf_vectors.sum(axis=0)\n",
    "data = []\n",
    "for col, term in enumerate(tfidf_terms):\n",
    "    data.append((term, sums[0,col]))\n",
    "frequency = pd.DataFrame(data, columns=['words', 'freq'])\n",
    "\n",
    "print(\"\\nNumber of words:\\n\", frequency.sort_values('freq', ascending=False)[:20])\n",
    "print(f\"Data samples: {tfidf_vectors.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(X):\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    X_reduced = svd.fit_transform(X)\n",
    "    return X_reduced\n",
    "\n",
    "X_reduced = reduce_dimensionality(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspection using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(X):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.01 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features in a scatter plot and coloring neighbor clusters differently\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.' , alpha=0.6)\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_tsne(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_clusters(X):\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "\n",
    "    # Taking a random sample of 5% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.05 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Define a range of clusters to test\n",
    "    cluster_range = range(2, 11)\n",
    "\n",
    "    for i in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=i, n_init=10, random_state=0)\n",
    "        kmeans.fit(X_sample)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(X_sample, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "        # Calculate Davies-Bouldin index\n",
    "        davies_bouldin_avg = davies_bouldin_score(X_sample, kmeans.labels_)\n",
    "        davies_bouldin_scores.append(davies_bouldin_avg)\n",
    "\n",
    "        # Print the score for each cluster\n",
    "        print(\"For n_clusters = {}, silhouette score is {})\".format(i, silhouette_avg))\n",
    "        print(\"For n_clusters = {}, Davies-Bouldin score is {})\".format(i, davies_bouldin_avg))\n",
    "\n",
    "\n",
    "    # Plot the scores \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Silhouette Score\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='-', color='g')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Plot Davies-Bouldin Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(cluster_range, davies_bouldin_scores, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Davies-Bouldin Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to determine optimal clusters\n",
    "determine_optimal_clusters(X_reduced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=0)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    return clusters\n",
    "\n",
    "optimal_clusters = 8  # Adjusted based on observation.\n",
    "df['cluster'] = apply_clustering(X_reduced, optimal_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each cluster most frequent words in a dataframe and show a table\n",
    "def print_cluster_words(df, vectorizer, cluster_number, top_n_words):\n",
    "    # Filter the dataframe to only get the rows associated with the given cluster\n",
    "    cluster_df = df[df['cluster'] == cluster_number]\n",
    "\n",
    "    # Get the text data from the dataframe\n",
    "    text_data = cluster_df['text']\n",
    "\n",
    "    # Instantiate the vectorizer\n",
    "    vectorizer = vectorizer\n",
    "\n",
    "    # Fit and transform the vectorizer on the text data\n",
    "    vectorized_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Create a list of the vectorized words\n",
    "    vectorized_data_as_array = vectorized_data.toarray()\n",
    "\n",
    "    # Create a DataFrame with the words\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    word_counts = pd.DataFrame({'word': vocab, 'count': np.sum(vectorized_data_as_array, axis=0)})\n",
    "    word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "    # Print the top n words from the DataFrame\n",
    "    print(word_counts.sort_values(by='count', ascending=False).head(top_n_words))\n",
    "\n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(' '.join(vocab))\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(optimal_clusters):\n",
    "    print_cluster_words(df, vectorizer, i, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(x='cluster', data=df)\n",
    "    plt.title('Distribution of Clusters')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(df)\n",
    "\n",
    "def visualize_clusters(X, clusters):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.01 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "    clusters_sample = clusters[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting in 3D\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_clusters(X_reduced, df['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/data_clustered.csv'\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
