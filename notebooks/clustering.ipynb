{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Dataset Clustering\n",
    "This notebook aims to cluster scholarly articles from the Arxiv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = '../data/data_preprocessed.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df):\n",
    "    # Replace NaN values in the 'text' column with empty strings\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    # Instantiate TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    return X, vectorizer\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample a fraction of data (e.g., 10%)\n",
    "sample_fraction = 0.1\n",
    "df_sample = df.sample(frac=sample_fraction)\n",
    "\n",
    "# Vectorize the sampled text data:\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\", analyzer=\"word\", ngram_range=(2,3), min_df=5, max_df=0.99, max_features=1500, lowercase=False)\n",
    "tfidf_vectors = vectorizer.fit_transform(df_sample['text'])\n",
    "tfidf_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Looking for most frequent words\n",
    "sums = tfidf_vectors.sum(axis=0)\n",
    "data = []\n",
    "\n",
    "# For each column, get the tuple (word, frequency)\n",
    "for col, word in enumerate(tfidf_words):\n",
    "    data.append((word, sums[0,col]))\n",
    "frequency = pd.DataFrame(data, columns=['words', 'freq'])\n",
    "\n",
    "print(\"\\nNumber of words:\\n\", frequency.sort_values('freq', ascending=False)[:20])\n",
    "print(f\"Data samples: {tfidf_vectors.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_dimensionality(X):\n",
    "    # Instantiate SVD\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    # Fit SVD to the data\n",
    "    svd.fit(X)\n",
    "    # Apply transform to the data\n",
    "    X_svd = svd.transform(X)\n",
    "    return X_svd\n",
    "\n",
    "X_svd = reduce_dimensionality(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspection using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(X):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.1 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features in a scatter plot and coloring neighbor clusters differently\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.' , alpha=0.6)\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_tsne(X_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum of squared distances for several k and plot the results:\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "within_cluster_squares={}\n",
    "for i in range(3,10):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42).fit(X_svd)\n",
    "    within_cluster_squares[i] = kmeans.inertia_\n",
    "plt.plot(list(within_cluster_squares.keys()),list(within_cluster_squares.values()))\n",
    "plt.xlabel('Values for K')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_clusters(X):\n",
    "    wcss = []\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "\n",
    "\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.1 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Define a range of clusters to test\n",
    "    cluster_range = range(2, 11)\n",
    "\n",
    "    for i in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=i, n_init=10, random_state=0)\n",
    "        kmeans.fit(X_sample)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(X_sample, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "        # Calculate Davies-Bouldin index\n",
    "        davies_bouldin_avg = davies_bouldin_score(X_sample, kmeans.labels_)\n",
    "        davies_bouldin_scores.append(davies_bouldin_avg)\n",
    "\n",
    "        # Print the score for each cluster\n",
    "        print(\"For n_clusters = {}, silhouette score is {})\".format(i, silhouette_avg))\n",
    "        print(\"For n_clusters = {}, Davies-Bouldin score is {})\".format(i, davies_bouldin_avg))\n",
    "\n",
    "\n",
    "    # Plot the scores \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Silhouette Score\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='-', color='g')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Plot Davies-Bouldin Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(cluster_range, davies_bouldin_scores, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Davies-Bouldin Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to determine optimal clusters\n",
    "determine_optimal_clusters(X_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    return clusters\n",
    "\n",
    "optimal_clusters = 8  # Adjusted based on observation.\n",
    "df['cluster'] = apply_clustering(X_svd, optimal_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each cluster most frequent words in a dataframe and show a table\n",
    "def print_cluster_words(df, vectorizer, cluster_number, top_n_words):\n",
    "    # Filter the dataframe to only get the rows associated with the given cluster\n",
    "    cluster_df = df[df['cluster'] == cluster_number]\n",
    "\n",
    "    # Get the text data from the dataframe\n",
    "    text_data = cluster_df['text']\n",
    "\n",
    "    # Instantiate the vectorizer\n",
    "    vectorizer = vectorizer\n",
    "\n",
    "    # Fit and transform the vectorizer on the text data\n",
    "    vectorized_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Create a list of the vectorized words\n",
    "    vectorized_data_as_array = vectorized_data.toarray()\n",
    "\n",
    "    # Create a DataFrame with the words\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    word_counts = pd.DataFrame({'word': vocab, 'count': np.sum(vectorized_data_as_array, axis=0)})\n",
    "    word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "    # Print the top n words from the DataFrame\n",
    "    print(word_counts.sort_values(by='count', ascending=False).head(top_n_words))\n",
    "\n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(' '.join(vocab))\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(optimal_clusters):\n",
    "    print_cluster_words(df, vectorizer, i, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(x='cluster', data=df)\n",
    "    plt.title('Distribution of Clusters')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(df)\n",
    "\n",
    "def visualize_clusters(X, clusters):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.01 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "    clusters_sample = clusters[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting in 3D\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_clusters(X_svd, df['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/data_clustered.csv'\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
