{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv Dataset Clustering\n",
    "This notebook aims to cluster scholarly articles from the Arxiv dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>categories</th>\n",
       "      <th>update_date</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hep-ph</td>\n",
       "      <td>2008-11-26</td>\n",
       "      <td>calculation prompt diphoton production section...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>math.CO cs.CG</td>\n",
       "      <td>2008-12-13</td>\n",
       "      <td>sparsity certifying decomposition algorithm el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>physics.gen-ph</td>\n",
       "      <td>2008-01-13</td>\n",
       "      <td>evolution earth moon dark matter field fluid e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>math.CO</td>\n",
       "      <td>2007-05-23</td>\n",
       "      <td>determinant stirling cycle number count unlabe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>math.CA math.FA</td>\n",
       "      <td>2013-10-15</td>\n",
       "      <td>dyadic lambda alpha lambda alpha compute lambd...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        categories update_date  \\\n",
       "0           hep-ph  2008-11-26   \n",
       "1    math.CO cs.CG  2008-12-13   \n",
       "2   physics.gen-ph  2008-01-13   \n",
       "3          math.CO  2007-05-23   \n",
       "4  math.CA math.FA  2013-10-15   \n",
       "\n",
       "                                                text  \n",
       "0  calculation prompt diphoton production section...  \n",
       "1  sparsity certifying decomposition algorithm el...  \n",
       "2  evolution earth moon dark matter field fluid e...  \n",
       "3  determinant stirling cycle number count unlabe...  \n",
       "4  dyadic lambda alpha lambda alpha compute lambd...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_file = '../data/data_preprocessed.csv'\n",
    "df = pd.read_csv(input_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(df):\n",
    "    # Replace NaN values in the 'text' column with empty strings\n",
    "    df['text'] = df['text'].fillna('')\n",
    "    # Instantiate TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(max_features=1000)\n",
    "    X = vectorizer.fit_transform(df['text'])\n",
    "    return X, vectorizer\n",
    "\n",
    "tfidf_matrix, vectorizer = vectorize_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of words:\n",
      "                        words         freq\n",
      "898           neural network  2957.562623\n",
      "751           magnetic field  2910.232164\n",
      "103               black hole  2508.787623\n",
      "744         machine learning  1862.690699\n",
      "243              dark matter  1687.519326\n",
      "859              monte carlo  1592.643635\n",
      "276            deep learning  1508.244009\n",
      "301    differential equation  1336.827307\n",
      "1005               power law  1119.432525\n",
      "285           degree freedom  1101.045916\n",
      "535                gamma ray  1064.858710\n",
      "820               mean field  1062.482127\n",
      "1131  reinforcement learning  1013.834744\n",
      "116       boundary condition  1002.603838\n",
      "1171            scalar field   943.386510\n",
      "1318          star formation   903.276962\n",
      "705              lie algebra   890.307058\n",
      "908             neutron star   859.392071\n",
      "1069        quantum mechanic   856.351726\n",
      "576       gravitational wave   824.612863\n",
      "Data samples: 228812\n"
     ]
    }
   ],
   "source": [
    "# Sample a fraction of data (e.g., 10%)\n",
    "sample_fraction = 0.1\n",
    "df_sample = df.sample(frac=sample_fraction)\n",
    "\n",
    "# Vectorize the sampled text data:\n",
    "vectorizer = TfidfVectorizer(norm=\"l2\", analyzer=\"word\", ngram_range=(2,3), min_df=5, max_df=0.99, max_features=1500, lowercase=False)\n",
    "tfidf_vectors = vectorizer.fit_transform(df_sample['text'])\n",
    "tfidf_words = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Looking for most frequent words\n",
    "sums = tfidf_vectors.sum(axis=0)\n",
    "data = []\n",
    "\n",
    "# For each column, get the tuple (word, frequency)\n",
    "for col, word in enumerate(tfidf_words):\n",
    "    data.append((word, sums[0,col]))\n",
    "frequency = pd.DataFrame(data, columns=['words', 'freq'])\n",
    "\n",
    "print(\"\\nNumber of words:\\n\", frequency.sort_values('freq', ascending=False)[:20])\n",
    "print(f\"Data samples: {tfidf_vectors.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tfidf_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\ML-trend-analysis\\Scientific-Trend-Analysis\\notebooks\\clustering.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML-trend-analysis/Scientific-Trend-Analysis/notebooks/clustering.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     X_svd \u001b[39m=\u001b[39m svd\u001b[39m.\u001b[39mtransform(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/ML-trend-analysis/Scientific-Trend-Analysis/notebooks/clustering.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m X_svd\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/ML-trend-analysis/Scientific-Trend-Analysis/notebooks/clustering.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_svd \u001b[39m=\u001b[39m reduce_dimensionality(tfidf_matrix)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tfidf_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "def reduce_dimensionality(X):\n",
    "    # Instantiate SVD\n",
    "    svd = TruncatedSVD(n_components=100, random_state=42)\n",
    "    # Fit SVD to the data\n",
    "    svd.fit(X)\n",
    "    # Apply transform to the data\n",
    "    X_svd = svd.transform(X)\n",
    "    return X_svd\n",
    "\n",
    "X_svd = reduce_dimensionality(tfidf_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Inspection using t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tsne(X):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.1 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features in a scatter plot and coloring neighbor clusters differently\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.' , alpha=0.6)\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_tsne(X_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine Optimal Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the sum of squared distances for several k\n",
    "fig = plt.figure(figsize=(8,8))\n",
    "wcss={}\n",
    "for i in range(2,11):\n",
    "    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42).fit(X_svd)\n",
    "    wcss[i] = kmeans.inertia_\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(list(wcss.keys()),list(wcss.values()))\n",
    "plt.xlabel('Values for K')\n",
    "plt.ylabel('Sum of Squared Distances')\n",
    "plt.title('Elbow Method')\n",
    "plt.grid(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_optimal_clusters(X):\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "\n",
    "\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.1 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "\n",
    "    # Define a range of clusters to test\n",
    "    cluster_range = range(2, 11)\n",
    "\n",
    "    for i in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=i, n_init=10, random_state=0)\n",
    "        kmeans.fit(X_sample)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        silhouette_avg = silhouette_score(X_sample, kmeans.labels_)\n",
    "        silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "\n",
    "        # Calculate Davies-Bouldin index\n",
    "        davies_bouldin_avg = davies_bouldin_score(X_sample, kmeans.labels_)\n",
    "        davies_bouldin_scores.append(davies_bouldin_avg)\n",
    "\n",
    "        # Print the score for each cluster\n",
    "        print(\"For n_clusters = {}, silhouette score is {})\".format(i, silhouette_avg))\n",
    "        print(\"For n_clusters = {}, Davies-Bouldin score is {})\".format(i, davies_bouldin_avg))\n",
    "\n",
    "\n",
    "    # Plot the scores \n",
    "    plt.figure(figsize=(15, 5))\n",
    "\n",
    "    # Plot Silhouette Score\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(cluster_range, silhouette_scores, marker='o', linestyle='-', color='g')\n",
    "    plt.title('Silhouette Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "\n",
    "    # Plot Davies-Bouldin Score\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(cluster_range, davies_bouldin_scores, marker='o', linestyle='-', color='r')\n",
    "    plt.title('Davies-Bouldin Score')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('Score')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Call the function to determine optimal clusters\n",
    "determine_optimal_clusters(X_svd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering(X, n_clusters):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n",
    "    clusters = kmeans.fit_predict(X)\n",
    "    return clusters\n",
    "\n",
    "optimal_clusters = 8  # Adjusted based on observation.\n",
    "df['cluster'] = apply_clustering(X_svd, optimal_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print each cluster most frequent words in a dataframe and show a table\n",
    "def print_cluster_words(df, vectorizer, cluster_number, top_n_words):\n",
    "    # Filter the dataframe to only get the rows associated with the given cluster\n",
    "    cluster_df = df[df['cluster'] == cluster_number]\n",
    "\n",
    "    # Get the text data from the dataframe\n",
    "    text_data = cluster_df['text']\n",
    "\n",
    "    # Instantiate the vectorizer\n",
    "    vectorizer = vectorizer\n",
    "\n",
    "    # Fit and transform the vectorizer on the text data\n",
    "    vectorized_data = vectorizer.fit_transform(text_data)\n",
    "\n",
    "    # Create a list of the vectorized words\n",
    "    vectorized_data_as_array = vectorized_data.toarray()\n",
    "\n",
    "    # Create a DataFrame with the words\n",
    "    vocab = vectorizer.get_feature_names()\n",
    "    word_counts = pd.DataFrame({'word': vocab, 'count': np.sum(vectorized_data_as_array, axis=0)})\n",
    "    word_counts = word_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "\n",
    "    # Print the top n words from the DataFrame\n",
    "    print(word_counts.sort_values(by='count', ascending=False).head(top_n_words))\n",
    "\n",
    "    # Create and generate a word cloud image\n",
    "    wordcloud = WordCloud().generate(' '.join(vocab))\n",
    "\n",
    "    # Display the generated image:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "for i in range(optimal_clusters):\n",
    "    print_cluster_words(df, vectorizer, i, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clusters(df):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.countplot(x='cluster', data=df)\n",
    "    plt.title('Distribution of Clusters')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Number of Samples')\n",
    "    plt.show()\n",
    "\n",
    "plot_clusters(df)\n",
    "\n",
    "def visualize_clusters(X, clusters):\n",
    "    # Taking a random sample of 1% of the data\n",
    "    sample_indices = np.random.choice(X.shape[0], int(0.01 * X.shape[0]), replace=False)\n",
    "    X_sample = X[sample_indices]\n",
    "    clusters_sample = clusters[sample_indices]\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "    # plotting in 3D\n",
    "\n",
    "    # Instantiating and fitting t-SNE\n",
    "    tsne = TSNE(n_components=3, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "\n",
    "    # Plotting the t-SNE features\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(X_tsne[:, 0], X_tsne[:, 1], X_tsne[:, 2], marker='.', c=clusters_sample, cmap='viridis')\n",
    "    plt.title('Data visualization using t-SNE')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualize_clusters(X_svd, df['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = '../data/data_clustered.csv'\n",
    "df.to_csv(output_file, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
